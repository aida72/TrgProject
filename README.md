# TrgProject

The workflow for deploying the Python Flask project contained in this repository leans heavily on the ideas of Continuous Integration (CI), as well as Continuous Delivery (CD).

In accordance with the principles of Continuous Integration, the project's source code repository is taken as a single possible source for a deployment.
```groovy

def  app = ''
pipeline {
    agent any
    stages {
            stage('Initialize') {
                steps {
                    script {
                        properties([pipelineTriggers([pollSCM('H * * * *')])])
                    }
                    git branch: "main", url: 'https://github.com/aida72/TrgProject.git'
                }
            }
            stage('Build image') {         
                steps {
                        script {
                            app = docker.build("123456odi/trgproject")   
                        }
                    }   
            }
            stage('Test image') { 
                steps {
                        script {          
                        app.inside {                 
                            sh 'python /code/test_hello_world.py'       
                        }    
                        }
                }
            }
            stage('Push image') {
                steps {
                    script {
                        docker.withRegistry('https://registry.hub.docker.com', 'dockerhub') {            
                            app.push("${env.BUILD_NUMBER}")            
                            app.push("latest") 
                        } 
                    }
                }  
            }
            stage('add ansible') {
                steps {
                    script {
                        ansiblePlaybook( 
                            playbook: 'ansible/playbook.yml'
                        )
                    }
                }
            }
        }
}

```
Once an update is pushed to the source code repository, Jenkins instance is notified using SCM Polling, which polls the commit and pull requests changes, and the CI/CD pipeline begins. The application docker image is built from the files pulled from the repository during the initialization stage and the tests are run inside the image. If the tests are successful, the application Docker image is built and pushed into a private Docker Hub repo. On success, Ansible playbook is run, deploying the Docker image to its target(in this case i mapped the docker demo of the host inside the container where ansible resides, since we cant run docker demo inside the container,  in order for ansible to use its docker plugin.). If successful, this concludes the Continuous Delivery part. 

The whole CI/CD pipeline is automatic and does not require human intervention unless there are problems with the tests.

## Future Work

### Introducing a reverse proxy for SSL traffic

Current deployment runs a very weak (default) Python-based web server to provide access to the application. Instead, an industrial-grade web-server, such as [nginx][nginx] can be deployed in its own container to provide the traffic ingress facilities, and to manage the SSL certificates.

Since that would increase the number of containers to deploy, it would be good to consider deployment with Kubernetes. Docker Compose is not really meant for production deployments.

[nginx]: https://www.nginx.com/

### Improving the security of SSL certificates

I used a self-signed certificate generated by openssl, for convinience just for the scope of the assignment, and because i was operating locally. It is against the best practices to use self-signed certificates. Commercially-provided SSL certificates are inexpensive and are harder to forge.


### Extending the scope of testing

After an update to the application is rolled out, a new step in CI/CD pipeline could conduct at least a _smoke test_ of the production deployment.

Another suite of tests that can be run against the deployed image is that of validating that SSL works, and that the application is not accessible outside its intended scope. The current intended scope is _localhost_

### Improving scalability and resilience

The current approach is not very suitable for high-load or resilient deployment because Docker is not a container orchestrator, so it does not scale the number of running instances as the load increases.

For this, a Kubernetes set-up would be a great task to focus on next.

The advantage of using Docker Compose is that it allows you to use a YAML file to operate multi-container applications at once. You can set the desired amount of containers counts, their builds, and storage designs, and then with a single set of commands you can build, run and configure all the containers.

Docker Compose is great for development, testing, and staging environments, as well as continuous integration workflows.


##Approach taken

I have created a Dockerfile for both instances of Jenkins and Ansible (i wanted to use ansible plugin for jenkins)
 and for this purpose i included both jenkins and ansible in the same container. Also by default jenkins is not secure, so i generated in the dockerfile a selfsigned certificate, and binded it to jenkins, just to make it https based. Its not the best approach but in production environment, we could use LetsEncrypt with certbot or any other commeercial certificate provided for jenkins. With the same principles as openssl but obviously more production-ready. 

I have intalled through Dockerfile the requuired packages for our setup python, ansible,  and other dependencies, based on jenkins:alpine linux. I didnt use a debian or ubuntu based because since they have more overhead on the storage, and jenkins:alpine takes up less  storage.

```dockerfile

FROM jenkins/jenkins:alpine

ENV CRYPTOGRAPHY_DONT_BUILD_RUST=1
COPY plugins.txt /usr/share/jenkins/plugins.txt
RUN /usr/local/bin/install-plugins.sh < /usr/share/jenkins/plugins.txt
USER root
RUN apk add --no-cache \
	bc \
    docker \
    py3-pip \
    python3-dev \
    libffi-dev \
    musl-dev \ 
    openssl-dev \ 
    gcc \ 
    libc-dev \ 
    make \
    curl \
	logrotate \
	nano \
	openssh \
    sudo \
    ansible \
	openrc

RUN apk add --no-cache openssl
RUN pip install docker-compose

RUN openssl genrsa -des3 -passout pass:aida -out server.pass.key 2048 && \
    openssl rsa -passin pass:aida -in server.pass.key -out jenkins.key && \
    rm server.pass.key && \
    openssl req -new -key jenkins.key -out jenkins.csr \
        -subj "/C=AL/ST=Tirana/L=Tirana/O=OrgName/OU=TRG/CN=localhost" && \
    openssl x509 -req -days 365 -in jenkins.csr -signkey jenkins.key -out jenkins.pem
    
RUN mkdir -p /var/lib/jenkins

RUN cp jenkins.pem /var/lib/jenkins/cert && \
    cp jenkins.key /var/lib/jenkins/pk && \
    chown jenkins:jenkins /var/lib/jenkins/cert && \
    chown jenkins:jenkins /var/lib/jenkins/pk

RUN apk add --no-cache python3

RUN pip install docker
RUN ansible-galaxy collection install community.docker

USER jenkins
ENV JENKINS_OPTS --httpPort=-1 --httpsPort=8088 --httpsCertificate=/var/lib/jenkins/cert --httpsPrivateKey=/var/lib/jenkins/pk
EXPOSE 8088



```
I built the container with command:

```bash

docker build -t jenkins_ansible_trg .

```

and i run it with 

```bash

sudo docker run --user root -v /var/run/docker.sock:/var/run/docker.sock -p 8088:8088 -p 8089:22 jenkins_ansible_trg


```
To persist your data:


```bash
jenkinsvol:/var/jenkins

```

THe explanation of using /var/run/docker.sock:/var/run/docker.sock
/var/run/docker.sock is a Unix domain socket. Sockets are used in your favorite Linux distro to allow different processes to communicate with one another. Like everything in Unix, sockets are files, too. In the case of Docker, /var/run/docker.sock is a way to communicate with the main Docker process and, because it's a file, we can share it with containers.


Also, I installed _Docker PLugin for Ansible in order to make possible the provisioning of docker image from dockerhub and run it in the destinated host. The playbook for ansible has two tasks, first one to login to dockerhub, and the second one is to run the container.
```yml
- hosts: localhost
  become: true

  tasks:
    - name: log into docker hub registry
      docker_login:
        email: "aida.liko@yahoo.com"
        username: "123456odi"
        password: "Admin1234"
       
    - name: ensure a container is running
      docker_container:
        name: trgproject
        state: started
        image: "123456odi/trgproject:latest"
        restart_policy: always
        pull: true
        ports:
          - "127.0.0.1:8087:8087"

```

```
Some commands to add new hosts in ansible:

```bash

docker build  -t my-docker-ssh .

docker run --rm -P -d  --name my-docker-ssh1  my-docker-ssh
docker run --rm -P -d  --name my-docker-ssh2  my-docker-ssh

```

 At the port section i exposed the application to port 8087 and also to make sure the app cant be accesed outside localhost of the host system, i restricted it to ip 127.0.0.1. SO this solves the part of the assigment which requires that the scope of app should be only on localhost.

Localhost inside docker means literally that, localhost, not all interfaces. But when you publish a port in bridge mode, it binds ports on external container interface to a port on the host.

```python
from flask import Flask
app = Flask( __name__ )

@app.route( "/")
def hello():
    return "Hello World!"

if __name__== "__main__" :
    
    context = ('/etc/ssl_certs/cert.pem', '/etc/ssl_certs/key.pem')
    app.run( host='0.0.0.0', port=8087,  ssl_context = context )

```

When we say  host=0.0.0.0 in our Flask app, it means bind on all interfaces.
So then, inside the container it's bound on both interfaces: that of the bridge network, as well as the localhost then when its --publish 127.0.0.1:8087:8087 that means (reading from right to left), container's port 8087 (container's "external" interface, not container's localhost!) to HOST's localhost, port 8087.
 



